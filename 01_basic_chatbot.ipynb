{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a01a95",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GenAIHub/agents-workshop/blob/main/01_basic_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1aae78-88a6-4133-b905-7e46c8e3772f",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "In this quick start, we will start with a basic chatbot that can answer common questions using an LLM, introducing key LangGraph concepts along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf95d6a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11d631-8679-4f28-822f-cdf1f2ddc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain langchain-core langchain-openai langchain-community \n",
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1e870-1bc0-4d44-86c0-96681ccf6113",
   "metadata": {},
   "source": [
    "Set API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d4020-6ee8-44cc-b1a5-8c34e7172fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://app-ads-sbx-openai-sw.openai.azure.com\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c374e41-f9b7-439e-a520-6d8c853c5220",
   "metadata": {},
   "source": [
    "# Build a Basic Chatbot\n",
    "\n",
    "We'll first create a simple chatbot using LangGraph, that will respond directly to user messages. Though simple, it will illustrate the core concepts of building with LangGraph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f927a07",
   "metadata": {},
   "source": [
    "## Start by creating a `StateGraph`. \n",
    "A `StateGraph` object defines the structure of our chatbot as a graph with a shared state.\n",
    "\n",
    "We'll add:\n",
    "- `nodes` to represent our agents\n",
    "- `edges` to specify how the bot should transition between these agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58df974-7579-4f25-9d91-66389b94eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages represents the chat history of our agent\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Define our graph with the given State\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c755cd-8994-4867-bdff-96a55d7beae7",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note:</p>\n",
    "    <p>\n",
    "    The first thing you do when you define a graph is define the <code>State</code> of the graph. \n",
    "    The <code>State</code> consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. In our example <code>State</code> is a <code>TypedDict</code> with a single key: <code>messages</code>. The <code>messages</code> key is annotated with the <a href=\"https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages\"><code>add_messages</code></a> reducer function, which tells LangGraph to append new messages to the existing list, rather than overwriting it. State keys without an annotation will be overwritten by each update, storing the most recent value.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137feed-746e-4c72-a34a-f7a699ad5dcf",
   "metadata": {},
   "source": [
    "So now our graph knows two things:\n",
    "\n",
    "1. Every `node` we define will receive the current `State` as input and return a value that updates that state.\n",
    "2. `messages` will be _appended_ to the current list, rather than directly overwritten. This is communicated via the prebuilt [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) function in the `Annotated` syntax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1410d",
   "metadata": {},
   "source": [
    "## Adding the Chatbot Node\n",
    "Nodes represent units of work. They are typically regular python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c9137-8261-42ea-8e83-3590981d23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Fetching environment variables\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "if not all([api_key, endpoint, api_version, deployment_name]):\n",
    "    raise ValueError(\"One or more environment variables are missing.\")\n",
    "\n",
    "# Initialize the Azure LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_deployment=deployment_name,\n",
    "    openai_api_version=api_version,\n",
    ")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # Invoke the LLM with the current chat history to get a response\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c1dcd9-fb86-4649-81b4-ff6ce20a2e46",
   "metadata": {},
   "source": [
    "**Notice** how the `chatbot` node function takes the current `State` as input and returns a dictionary containing an updated `messages` list under the key \"messages\". This is the basic pattern for all LangGraph node functions.\n",
    "\n",
    "The `add_messages` function in our `State` will append the llm's response messages to whatever messages are already in the state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902d273",
   "metadata": {},
   "source": [
    "## Setting the Entry and Finish Points\n",
    "Next, add an `entry` point. This tells our graph **where to start its work** each time we run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331e10d-ebcf-4144-9bd3-999b4d656dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499c318-d1e6-46fa-a652-8f9e65313355",
   "metadata": {},
   "source": [
    "Similarly, set a `finish` point. This instructs the graph **\"any time this node is run, you can exit.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f0929-3591-4852-b2d3-eaadde40662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9b88c-2c53-4d95-8eb1-d544a8946f65",
   "metadata": {},
   "source": [
    "## Compile the Graph\n",
    "Finally, we'll want to be able to run our graph. To do so, call \"`compile()`\" on the graph builder. This creates a \"`CompiledGraph`\" we can use invoke on our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb67a01-cf5c-4625-8c07-6e8c0af50fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39407b-d6f6-48a4-b1f6-31fc7f88b275",
   "metadata": {},
   "source": [
    "## Visualizing the Graph\n",
    "You can visualize the graph using the `get_graph` method and one of the \"draw\" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4f36e-72ce-4ade-bd7e-94880e0d456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98097a3-a126-4081-b21e-697ec1185fff",
   "metadata": {},
   "source": [
    "## Now let's run the chatbot! \n",
    "\n",
    "**Tip:** You can exit the chat loop at any time by typing \"quit\", \"exit\", or \"q\". <br>\n",
    "**Note:** This implementation does not keep track of chat history though different calls of the graph. As a result, the system does not remember previous interactions with the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb4c9a-7404-4e92-9945-36f372015f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Take user input\n",
    "    user_input = input(\"\\n\\nUser: \")\n",
    "    # Check whether or not to exit the loop\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    # Call the graph with a 'State' object containing the user input\n",
    "    # The chat history of the previous calls to the graph is not passed!\n",
    "    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f3f11",
   "metadata": {},
   "source": [
    "## Exercise (optional):\n",
    "\n",
    "Extend the code above such that the chat history of previous graph calls is passed to the next graph call.\n",
    "As an end-user, you should be able to ask questions regarding previous interactions if implemented correctly.\n",
    "You may test your solution by stating something in the first interaction \n",
    "and asking the system to repeat what you said in the second interaction. <br>\n",
    "<br> **Hint:** *value[\"messages\"]* does not return the entire chat history of a call to the graph. Instead it only returns the responses of the calls to the LLM. So you should ensure that you add the user_input to the chat history first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e722196",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "Your answer may differ as multiple solutions exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the histroy so far\n",
    "# Start with an empty history\n",
    "history = {\"messages\": []}\n",
    "\n",
    "while True:\n",
    "    # Take user input\n",
    "    user_input = input(\"\\n\\nUser: \")\n",
    "    # Add the user input to the history\n",
    "    history['messages'] += [(\"user\", user_input)] \n",
    "    # Check whether or not to exit the loop\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    # Take the chat histroy by accessing the messages in its state\n",
    "    history_messages = history[\"messages\"] \n",
    "    # Pass the entire conversation history to the graph\n",
    "    for event in graph.stream({\"messages\": history_messages}): \n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "            # Add the messages present in the event to the chat history\n",
    "            # These messages only contain the response of the LLM\n",
    "            history['messages'] += value['messages'] \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afe0eb",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "Keeping track of the history with your multi-agent system manually is a hassle. Luckily, LangGraph provides an abstraction such that it manages and keeps track of its memory across different invocations by itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# The config allows our system to run interactions with different users \n",
    "# and still keep track of the memory for each user separately!\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Take user input\n",
    "    user_input = input(\"\\n\\nUser: \")\n",
    "    # Check whether or not to exit the loop\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    # The chat history of the previous calls to the graph is not passed!\n",
    "    # But due to the memory saver, the system will now manage its own memory!\n",
    "    # we run each graph call with the same config, as all interactions are performed by the same end-user.\n",
    "    for event in graph.stream({\"messages\": (\"user\", user_input)}, config):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1cdb5-869a-41ea-9dab-e28cfc524499",
   "metadata": {},
   "source": [
    "## **Congratulations!** \n",
    "You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. \n",
    "\n",
    "However, you may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll add a web search tool to expand the bot's knowledge and make it more capable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
