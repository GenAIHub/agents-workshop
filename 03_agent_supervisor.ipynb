{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f09bf0b9",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/GenAIHub/agents-workshop/blob/main/03_agent_supervisor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276",
      "metadata": {
        "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276"
      },
      "source": [
        "## Agent Supervisor\n",
        "\n",
        "In below example, we will create an agent group, with an agent supervisor to help delegate tasks.\n",
        "\n",
        "![diagram](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/img/supervisor-diagram.png?raw=1)\n",
        "\n",
        "To simplify the code in each agent node, we will use the AgentExecutor class from LangChain.\n",
        "\n",
        "Before we build, let's configure our environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d30b6f7-3bec-4d9f-af50-43dfdc81ae6c",
      "metadata": {
        "id": "0d30b6f7-3bec-4d9f-af50-43dfdc81ae6c"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph\n",
        "%pip install -U langchain langchain_openai langchain_experimental langsmith pandas\n",
        "%pip install -U tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a89566",
      "metadata": {
        "id": "53a89566"
      },
      "source": [
        "### Set API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30c2f3de-c730-4aec-85a6-af2c2f058803",
      "metadata": {
        "id": "30c2f3de-c730-4aec-85a6-af2c2f058803"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://app-ads-sbx-openai-sw.openai.azure.com\"\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a024b3",
      "metadata": {
        "id": "38a024b3"
      },
      "source": [
        "### Initialize the Azure LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d3c644f",
      "metadata": {
        "id": "6d3c644f"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "# Fetching environment variables\n",
        "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
        "\n",
        "if not all([api_key, endpoint, api_version, deployment_name]):\n",
        "    raise ValueError(\"One or more environment variables are missing.\")\n",
        "\n",
        "# Initialize the Azure LLM\n",
        "llm = AzureChatOpenAI(\n",
        "    openai_api_key=api_key,\n",
        "    azure_endpoint=endpoint,\n",
        "    azure_deployment=deployment_name,\n",
        "    openai_api_version=api_version,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac25624-4d83-45a4-b9ef-a10589aacfb7",
      "metadata": {
        "id": "1ac25624-4d83-45a4-b9ef-a10589aacfb7"
      },
      "source": [
        "### Create tools\n",
        "\n",
        "For this example, we will make an agent to do web research with a search engine, and one agent to create plots by executing python code. Define the tools they'll use below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04c6778-403b-4b49-9b93-678e910d5cec",
      "metadata": {
        "id": "f04c6778-403b-4b49-9b93-678e910d5cec"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Import the prebuilt Tavily Tool\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "\n",
        "# Let's build the code executor agent ourselves!\n",
        "\n",
        "# This executes code locally, which can be unsafe\n",
        "def execute_python_code(code):\n",
        "    try:\n",
        "        # Create a dictionary to capture the local variables\n",
        "        local_vars = {}\n",
        "        \n",
        "        # Execute the code in the current notebook environment\n",
        "        exec(code, globals(), local_vars)\n",
        "        \n",
        "        # If no string were to be returned, our supervisor would have no idea whether or not the code execution was successful.\n",
        "        return \"Done plotting figure.\"\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce478de",
      "metadata": {},
      "source": [
        "#### **Question (optional):**\n",
        "\n",
        "Create a tool from the function described above. <br>\n",
        "You may base yourself on how we build the dall-e tool in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e06348",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Input Model\n",
        "# To be implemented\n",
        "\n",
        "### Tool\n",
        "# To be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee0013c",
      "metadata": {},
      "source": [
        "#### **Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a147ce63",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CodeExecutorInput(BaseModel):\n",
        "    code: str = Field(\n",
        "        description=\"The python code to be executed.\"\n",
        "    )\n",
        "\n",
        "python_repl_tool = StructuredTool.from_function(\n",
        "    func=execute_python_code,\n",
        "    # The name does not have to be the function name, but make sure that it is a relevant name as the LLM also makes use of it to decide its course of action.\n",
        "    name=\"execute_python_code\",\n",
        "    # It is important to specify that the code should display the plot. Otherwise the generated code may try to save the plot instead.\n",
        "    description=\"Execute Python code to display plots.\",\n",
        "    args_schema=CodeExecutorInput,\n",
        "    return_direct=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d58d1e85-22d4-4c22-9062-72a346a0d709",
      "metadata": {
        "id": "d58d1e85-22d4-4c22-9062-72a346a0d709"
      },
      "source": [
        "### Helper Utilities\n",
        "\n",
        "Define a helper function below, which make it easier to add new agent worker nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4823dd9-26bd-4e1a-8117-b97b2860211a",
      "metadata": {
        "id": "c4823dd9-26bd-4e1a-8117-b97b2860211a"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "def create_agent(llm: AzureChatOpenAI, tools: list, system_prompt: str):\n",
        "    # The prompt should include: the system prompt, the chat history and an overview of all available tools that can be used by the agent.\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "    # Prebuilt tool to create an openai agent with tools.\n",
        "    # This agent generates a step-by-step plan that can be executed with the AgentExecutor.\n",
        "    # The AgentExecutor itself will handle the logic to call the correct tool, no need for a seperate 'tools' node.\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c302b0-cd57-4913-986f-5dc7d6d77386",
      "metadata": {
        "id": "b7c302b0-cd57-4913-986f-5dc7d6d77386"
      },
      "source": [
        "We can also define a function that we will use to be the nodes in the graph - it takes care of converting the agent response to a human message. This is important because that is how we will add it the global state of the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80862241-a1a7-4726-bce5-f867b233832e",
      "metadata": {
        "id": "80862241-a1a7-4726-bce5-f867b233832e"
      },
      "outputs": [],
      "source": [
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32962d2-5487-496d-aefc-2a3b0d194985",
      "metadata": {
        "id": "d32962d2-5487-496d-aefc-2a3b0d194985"
      },
      "source": [
        "### Create Agent Supervisor\n",
        "\n",
        "It will use function calling to choose the next worker node OR finish processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311f0a58-b425-4496-adac-dc4cd8ffb912",
      "metadata": {
        "id": "311f0a58-b425-4496-adac-dc4cd8ffb912"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define members\n",
        "members = [\"Researcher\", \"Coder\"]\n",
        "\n",
        "# Define the system prompt\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers:  {members}. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When finished,\"\n",
        "    \" respond with FINISH.\"\n",
        ")\n",
        "\n",
        "# Define the routing options\n",
        "options = [\"FINISH\"] + members\n",
        "\n",
        "# Define the function that the supervisor will use.\n",
        "# In this case we directly define the function in its JSON representation.\n",
        "function_def = {\n",
        "    \"name\": \"route\",\n",
        "    \"description\": \"Select the next role.\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"routeSchema\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"next\": {\n",
        "                \"title\": \"Next\",\n",
        "                \"anyOf\": [\n",
        "                    {\"enum\": options},\n",
        "                ],\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"next\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Create the supervisor prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Given the conversation above, who should act next?\"\n",
        "            \" Or should we FINISH? Select one of: {options}\",\n",
        "        ),\n",
        "    ]\n",
        "# Fix the options variable and the members variable defined in the system_prompt above.\n",
        ").partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "# Create the supervisor chain using the Azure LLM\n",
        "# Chains will call each function in the chain with the output of the function that comes before them in the chain.\n",
        "# The first function will be called with the input that is given when the chain is invoked.\n",
        "supervisor_chain = (\n",
        "    prompt\n",
        "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "    | JsonOutputFunctionsParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07d507f-34d1-4f1b-8dde-5e58d17b2166",
      "metadata": {
        "id": "a07d507f-34d1-4f1b-8dde-5e58d17b2166"
      },
      "source": [
        "### Construct Graph\n",
        "\n",
        "We're ready to start building the graph. Below, define the state and worker nodes using the function we just defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a430af7-8fce-4e66-ba9e-d940c1bc48e8",
      "metadata": {
        "id": "6a430af7-8fce-4e66-ba9e-d940c1bc48e8"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import operator\n",
        "from typing import Sequence, TypedDict\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "# Define the AgentState\n",
        "# We add a next element to our state as the output of our supervisor will automatically overwrite this whenever it called \n",
        "# due to the function (function_def) we bound to it earlier.\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    next: str\n",
        "\n",
        "# Create the agents\n",
        "research_agent = create_agent(llm, [tavily_tool], \"You are a web researcher.\")\n",
        "# Fix the agent and name arguments of the agent_node functions. This way it can be invoked only using the 'state' later on.\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
        "\n",
        "code_agent = create_agent(\n",
        "    llm,\n",
        "    [python_repl_tool],\n",
        "    \"You may generate safe Python code to analyze data and generate charts using matplotlib.\",\n",
        ")\n",
        "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
        "\n",
        "# Build the workflow\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "# Each agent is a seperate node\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"Coder\", code_node)\n",
        "workflow.add_node(\"supervisor\", supervisor_chain)\n",
        "\n",
        "# Add edges\n",
        "# Whenever either the Coder or Researcher agent is finished, it should return to the supervisor agent.\n",
        "for member in members:\n",
        "    workflow.add_edge(member, \"supervisor\")\n",
        "\n",
        "# Define conditional routing from supervisor\n",
        "conditional_map = {k: k for k in members}\n",
        "conditional_map[\"FINISH\"] = END\n",
        "# Look at the 'next' in our state and route the flow to said agent.\n",
        "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
        "\n",
        "# Add the entry point\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "\n",
        "# Compile the graph (do this only once after all nodes and edges are added)\n",
        "graph = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36496de-7121-4c49-8cb6-58c943c66628",
      "metadata": {
        "id": "d36496de-7121-4c49-8cb6-58c943c66628"
      },
      "source": [
        "### Invoke the team"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "awnzXor_T-xh",
      "metadata": {
        "id": "awnzXor_T-xh"
      },
      "source": [
        "The following example should let the system look up the current populations in LA and NY by using the Research Agent. Afterward, it should plot and display these values with the Coder Agent. Feel free to try other queries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a92dfd-0e11-47f5-aad4-b68d24990e34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45a92dfd-0e11-47f5-aad4-b68d24990e34",
        "outputId": "df1bd8d9-8f1b-43e1-bb65-1ded5044cbbc"
      },
      "outputs": [],
      "source": [
        "message = \"Wat is the current population in LA? Generate and display a plot that compares said population to the current population of NY.\"\n",
        "\n",
        "for s in graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=message)]},\n",
        "    {\"recursion_limit\": 100},\n",
        "):\n",
        "    if \"__end__\" not in s:\n",
        "        print(s)\n",
        "        print(\"----\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
